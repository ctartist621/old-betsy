{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is for the model to take:\n",
    "\n",
    "1) Time\n",
    "2) Transmit Grid Square\n",
    "\n",
    "Output:\n",
    "1) Most likely Grid Squares to contact\n",
    "2) Which band to use\n",
    "3) Which Mode to use\n",
    "\n",
    "Label is: db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.transforms import LabelToIndex\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DXDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_csv(file)\n",
    "\n",
    "        # self.data = self.data.sample(frac=0.001)\n",
    "        self.labels = self.data['callsign_grid'].unique()\n",
    "\n",
    "        self.target_labels = {\n",
    "            'dx_grid': self.data['dx_grid'].unique(),\n",
    "            'band': self.data['band'].unique(),\n",
    "            'mode': self.data['mode'].unique(),\n",
    "            'tx_mode': self.data['tx_mode'].unique(),\n",
    "        }\n",
    "\n",
    "        self.transform = LabelToIndex(label_names=self.labels.tolist())\n",
    "        self.target_transforms = {\n",
    "            'dx_grid': LabelToIndex(label_names=self.target_labels['dx_grid'].tolist()),\n",
    "            'band': LabelToIndex(label_names=self.target_labels['band'].tolist()),\n",
    "            'mode': LabelToIndex(label_names=self.target_labels['mode'].tolist()),\n",
    "            'tx_mode': LabelToIndex(label_names=self.target_labels['tx_mode'].tolist()),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx].drop([\"Unnamed: 0\", \"snr\",'dx_grid','band','mode', 'tx_mode'])\n",
    "        label = self.data.iloc[idx]\n",
    "\n",
    "        data['callsign_grid'] = self.transform.forward(data['callsign_grid'])\n",
    "        data = torch.FloatTensor(data)\n",
    "        label = torch.FloatTensor([\n",
    "            self.target_transforms['dx_grid'].forward(label['dx_grid']),\n",
    "            self.target_transforms['band'].forward(label['band']),\n",
    "            self.target_transforms['mode'].forward(label['mode']),\n",
    "            self.target_transforms['tx_mode'].forward(label['tx_mode']),\n",
    "        ])\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285077 71723\n"
     ]
    }
   ],
   "source": [
    "training_data = DXDataset(\"data/train.csv\")\n",
    "test_data = DXDataset(\"data/test.csv\")\n",
    "print(len(training_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44.,  2.,  0.,  0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data, label = training_data.__getitem__(300)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60.,  -1.,  -1.,  ...,   2.,  35.,  56.],\n",
       "        [130.,  -1.,  -1.,  ...,  11.,  50.,  26.],\n",
       "        [  9.,  -1.,  -1.,  ...,  11.,  28.,  35.],\n",
       "        ...,\n",
       "        [ 11.,  -1.,  -1.,  ...,   9.,  27.,  52.],\n",
       "        [ 57.,  -1.,  -1.,  ...,   2.,  33.,  42.],\n",
       "        [ 41.,  -1.,  -1.,  ...,   9.,  26.,  12.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 51])\n",
      "Labels batch shape: torch.Size([64, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([1087.,    3.,    0.,    0.])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "input = train_features[0]\n",
    "label = train_labels[0]\n",
    "plt.plot(input)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(len(training_data.__getitem__(0)[0]), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, len(training_data.__getitem__(0)[1])),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            item_check = (pred.argmax(dim=0) == y)\n",
    "\n",
    "            correct += (pred.argmax(dim=0) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= (size * y.numel() \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 20129.039062  [    0/285077]\n",
      "loss: 527609.187500  [ 6400/285077]\n",
      "loss: 244442.953125  [12800/285077]\n",
      "loss: 55528.382812  [19200/285077]\n",
      "loss: 157270.484375  [25600/285077]\n",
      "loss: 43845.664062  [32000/285077]\n",
      "loss: 230290.109375  [38400/285077]\n",
      "loss: 101550.296875  [44800/285077]\n",
      "loss: 46639.453125  [51200/285077]\n",
      "loss: 145.615952  [57600/285077]\n",
      "loss: 23.554371  [64000/285077]\n",
      "loss: 22.658058  [70400/285077]\n",
      "loss: 21.969233  [76800/285077]\n",
      "loss: 19.049816  [83200/285077]\n",
      "loss: 20.043266  [89600/285077]\n",
      "loss: 19.719313  [96000/285077]\n",
      "loss: 17.962652  [102400/285077]\n",
      "loss: 19.513931  [108800/285077]\n",
      "loss: 18.459335  [115200/285077]\n",
      "loss: 19.323519  [121600/285077]\n",
      "loss: 14.575040  [128000/285077]\n",
      "loss: 17.032549  [134400/285077]\n",
      "loss: 16.449919  [140800/285077]\n",
      "loss: 18.817337  [147200/285077]\n",
      "loss: 17.731905  [153600/285077]\n",
      "loss: 18.923172  [160000/285077]\n",
      "loss: 18.029598  [166400/285077]\n",
      "loss: 19.056351  [172800/285077]\n",
      "loss: 16.909197  [179200/285077]\n",
      "loss: 18.945704  [185600/285077]\n",
      "loss: 19.111021  [192000/285077]\n",
      "loss: 17.955566  [198400/285077]\n",
      "loss: 21.892172  [204800/285077]\n",
      "loss: 16.507946  [211200/285077]\n",
      "loss: 18.178881  [217600/285077]\n",
      "loss: 15.323453  [224000/285077]\n",
      "loss: 15.830706  [230400/285077]\n",
      "loss: 18.455303  [236800/285077]\n",
      "loss: 17.603313  [243200/285077]\n",
      "loss: 19.085598  [249600/285077]\n",
      "loss: 15.431190  [256000/285077]\n",
      "loss: 17.301979  [262400/285077]\n",
      "loss: 15.971650  [268800/285077]\n",
      "loss: 15.865715  [275200/285077]\n",
      "loss: 17.372742  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 14.643889 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 15.925323  [    0/285077]\n",
      "loss: 16.912369  [ 6400/285077]\n",
      "loss: 16.945934  [12800/285077]\n",
      "loss: 17.499783  [19200/285077]\n",
      "loss: 19.916901  [25600/285077]\n",
      "loss: 16.074993  [32000/285077]\n",
      "loss: 18.137270  [38400/285077]\n",
      "loss: 17.406542  [44800/285077]\n",
      "loss: 17.792427  [51200/285077]\n",
      "loss: 17.769306  [57600/285077]\n",
      "loss: 15.700748  [64000/285077]\n",
      "loss: 15.799955  [70400/285077]\n",
      "loss: 17.445559  [76800/285077]\n",
      "loss: 17.484106  [83200/285077]\n",
      "loss: 15.412538  [89600/285077]\n",
      "loss: 17.383154  [96000/285077]\n",
      "loss: 16.569878  [102400/285077]\n",
      "loss: 17.898907  [108800/285077]\n",
      "loss: 19.787167  [115200/285077]\n",
      "loss: 20.405914  [121600/285077]\n",
      "loss: 15.313108  [128000/285077]\n",
      "loss: 20.162355  [134400/285077]\n",
      "loss: 16.399673  [140800/285077]\n",
      "loss: 18.610287  [147200/285077]\n",
      "loss: 17.585512  [153600/285077]\n",
      "loss: 16.390720  [160000/285077]\n",
      "loss: 15.984182  [166400/285077]\n",
      "loss: 13.910346  [172800/285077]\n",
      "loss: 20.502602  [179200/285077]\n",
      "loss: 27.472662  [185600/285077]\n",
      "loss: 17.792511  [192000/285077]\n",
      "loss: 23.333086  [198400/285077]\n",
      "loss: 18.454350  [204800/285077]\n",
      "loss: 14.433399  [211200/285077]\n",
      "loss: 21.184090  [217600/285077]\n",
      "loss: 25.481483  [224000/285077]\n",
      "loss: 16.347288  [230400/285077]\n",
      "loss: 17.820784  [236800/285077]\n",
      "loss: 16.792019  [243200/285077]\n",
      "loss: 19.710812  [249600/285077]\n",
      "loss: 17.179295  [256000/285077]\n",
      "loss: 16.229527  [262400/285077]\n",
      "loss: 16.219721  [268800/285077]\n",
      "loss: 18.475368  [275200/285077]\n",
      "loss: 14.341535  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 14.974993 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 16.665569  [    0/285077]\n",
      "loss: 19.194908  [ 6400/285077]\n",
      "loss: 16.855932  [12800/285077]\n",
      "loss: 16.783775  [19200/285077]\n",
      "loss: 16.579210  [25600/285077]\n",
      "loss: 16.517786  [32000/285077]\n",
      "loss: 18.594250  [38400/285077]\n",
      "loss: 16.397623  [44800/285077]\n",
      "loss: 17.863556  [51200/285077]\n",
      "loss: 15.477072  [57600/285077]\n",
      "loss: 18.707472  [64000/285077]\n",
      "loss: 15.558825  [70400/285077]\n",
      "loss: 16.195656  [76800/285077]\n",
      "loss: 18.121689  [83200/285077]\n",
      "loss: 19.361952  [89600/285077]\n",
      "loss: 14.977783  [96000/285077]\n",
      "loss: 17.381990  [102400/285077]\n",
      "loss: 17.456863  [108800/285077]\n",
      "loss: 17.099398  [115200/285077]\n",
      "loss: 15.194944  [121600/285077]\n",
      "loss: 16.630518  [128000/285077]\n",
      "loss: 17.287682  [134400/285077]\n",
      "loss: 14.845664  [140800/285077]\n",
      "loss: 15.058954  [147200/285077]\n",
      "loss: 17.476522  [153600/285077]\n",
      "loss: 18.697205  [160000/285077]\n",
      "loss: 21.346865  [166400/285077]\n",
      "loss: 18.697769  [172800/285077]\n",
      "loss: 23.086554  [179200/285077]\n",
      "loss: 18.385025  [185600/285077]\n",
      "loss: 17.770826  [192000/285077]\n",
      "loss: 15.503468  [198400/285077]\n",
      "loss: 16.911049  [204800/285077]\n",
      "loss: 16.802412  [211200/285077]\n",
      "loss: 16.501808  [217600/285077]\n",
      "loss: 19.127502  [224000/285077]\n",
      "loss: 18.699800  [230400/285077]\n",
      "loss: 17.732328  [236800/285077]\n",
      "loss: 16.111715  [243200/285077]\n",
      "loss: 15.452332  [249600/285077]\n",
      "loss: 16.989576  [256000/285077]\n",
      "loss: 17.678122  [262400/285077]\n",
      "loss: 19.270845  [268800/285077]\n",
      "loss: 15.516809  [275200/285077]\n",
      "loss: 15.783284  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 14.826169 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 17.860134  [    0/285077]\n",
      "loss: 17.590807  [ 6400/285077]\n",
      "loss: 16.492062  [12800/285077]\n",
      "loss: 20.386576  [19200/285077]\n",
      "loss: 15.598292  [25600/285077]\n",
      "loss: 15.229179  [32000/285077]\n",
      "loss: 16.556461  [38400/285077]\n",
      "loss: 17.437984  [44800/285077]\n",
      "loss: 18.272469  [51200/285077]\n",
      "loss: 16.271101  [57600/285077]\n",
      "loss: 16.640760  [64000/285077]\n",
      "loss: 16.785942  [70400/285077]\n",
      "loss: 16.842714  [76800/285077]\n",
      "loss: 21.959681  [83200/285077]\n",
      "loss: 16.184416  [89600/285077]\n",
      "loss: 17.477640  [96000/285077]\n",
      "loss: 17.129883  [102400/285077]\n",
      "loss: 17.437355  [108800/285077]\n",
      "loss: 16.864271  [115200/285077]\n",
      "loss: 15.241679  [121600/285077]\n",
      "loss: 14.730807  [128000/285077]\n",
      "loss: 18.986233  [134400/285077]\n",
      "loss: 17.352304  [140800/285077]\n",
      "loss: 16.000202  [147200/285077]\n",
      "loss: 16.140629  [153600/285077]\n",
      "loss: 17.759527  [160000/285077]\n",
      "loss: 16.608255  [166400/285077]\n",
      "loss: 17.033422  [172800/285077]\n",
      "loss: 15.994148  [179200/285077]\n",
      "loss: 17.993784  [185600/285077]\n",
      "loss: 16.209555  [192000/285077]\n",
      "loss: 16.880711  [198400/285077]\n",
      "loss: 17.283802  [204800/285077]\n",
      "loss: 18.956003  [211200/285077]\n",
      "loss: 18.078972  [217600/285077]\n",
      "loss: 16.958933  [224000/285077]\n",
      "loss: 17.861488  [230400/285077]\n",
      "loss: 17.065845  [236800/285077]\n",
      "loss: 17.034168  [243200/285077]\n",
      "loss: 16.316551  [249600/285077]\n",
      "loss: 22.901047  [256000/285077]\n",
      "loss: 18.386587  [262400/285077]\n",
      "loss: 16.886206  [268800/285077]\n",
      "loss: 16.899399  [275200/285077]\n",
      "loss: 17.611176  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 14.805707 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 15.974640  [    0/285077]\n",
      "loss: 19.943657  [ 6400/285077]\n",
      "loss: 18.384972  [12800/285077]\n",
      "loss: 19.335262  [19200/285077]\n",
      "loss: 14.461150  [25600/285077]\n",
      "loss: 15.679498  [32000/285077]\n",
      "loss: 18.086121  [38400/285077]\n",
      "loss: 16.777615  [44800/285077]\n",
      "loss: 16.927896  [51200/285077]\n",
      "loss: 17.210386  [57600/285077]\n",
      "loss: 17.935991  [64000/285077]\n",
      "loss: 17.711334  [70400/285077]\n",
      "loss: 16.030581  [76800/285077]\n",
      "loss: 15.937954  [83200/285077]\n",
      "loss: 19.411858  [89600/285077]\n",
      "loss: 19.110931  [96000/285077]\n",
      "loss: 16.016903  [102400/285077]\n",
      "loss: 19.484934  [108800/285077]\n",
      "loss: 14.642879  [115200/285077]\n",
      "loss: 15.238591  [121600/285077]\n",
      "loss: 16.875198  [128000/285077]\n",
      "loss: 14.704466  [134400/285077]\n",
      "loss: 16.883120  [140800/285077]\n",
      "loss: 20.494375  [147200/285077]\n",
      "loss: 17.231853  [153600/285077]\n",
      "loss: 16.726227  [160000/285077]\n",
      "loss: 16.495434  [166400/285077]\n",
      "loss: 14.360735  [172800/285077]\n",
      "loss: 17.604044  [179200/285077]\n",
      "loss: 16.690332  [185600/285077]\n",
      "loss: 18.588402  [192000/285077]\n",
      "loss: 18.339651  [198400/285077]\n",
      "loss: 15.489207  [204800/285077]\n",
      "loss: 18.574165  [211200/285077]\n",
      "loss: 17.690720  [217600/285077]\n",
      "loss: 16.917036  [224000/285077]\n",
      "loss: 16.589195  [230400/285077]\n",
      "loss: 15.392841  [236800/285077]\n",
      "loss: 18.632965  [243200/285077]\n",
      "loss: 14.676572  [249600/285077]\n",
      "loss: 15.009282  [256000/285077]\n",
      "loss: 18.136284  [262400/285077]\n",
      "loss: 18.005001  [268800/285077]\n",
      "loss: 15.564320  [275200/285077]\n",
      "loss: 19.950846  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 15.142001 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 18.923306  [    0/285077]\n",
      "loss: 17.689041  [ 6400/285077]\n",
      "loss: 20.272900  [12800/285077]\n",
      "loss: 22.861874  [19200/285077]\n",
      "loss: 16.772758  [25600/285077]\n",
      "loss: 16.319086  [32000/285077]\n",
      "loss: 15.607050  [38400/285077]\n",
      "loss: 16.235312  [44800/285077]\n",
      "loss: 15.905572  [51200/285077]\n",
      "loss: 16.095604  [57600/285077]\n",
      "loss: 16.226353  [64000/285077]\n",
      "loss: 16.989986  [70400/285077]\n",
      "loss: 15.903466  [76800/285077]\n",
      "loss: 21.420324  [83200/285077]\n",
      "loss: 16.617950  [89600/285077]\n",
      "loss: 19.101995  [96000/285077]\n",
      "loss: 15.527149  [102400/285077]\n",
      "loss: 15.871630  [108800/285077]\n",
      "loss: 16.813564  [115200/285077]\n",
      "loss: 20.677944  [121600/285077]\n",
      "loss: 16.550495  [128000/285077]\n",
      "loss: 18.827253  [134400/285077]\n",
      "loss: 19.226820  [140800/285077]\n",
      "loss: 17.236038  [147200/285077]\n",
      "loss: 14.063250  [153600/285077]\n",
      "loss: 17.816164  [160000/285077]\n",
      "loss: 18.297791  [166400/285077]\n",
      "loss: 17.382902  [172800/285077]\n",
      "loss: 21.656122  [179200/285077]\n",
      "loss: 15.081149  [185600/285077]\n",
      "loss: 17.379351  [192000/285077]\n",
      "loss: 17.126352  [198400/285077]\n",
      "loss: 16.818680  [204800/285077]\n",
      "loss: 18.392788  [211200/285077]\n",
      "loss: 23.365013  [217600/285077]\n",
      "loss: 19.806643  [224000/285077]\n",
      "loss: 16.284746  [230400/285077]\n",
      "loss: 19.381924  [236800/285077]\n",
      "loss: 16.396320  [243200/285077]\n",
      "loss: 16.028160  [249600/285077]\n",
      "loss: 15.656670  [256000/285077]\n",
      "loss: 17.328571  [262400/285077]\n",
      "loss: 15.092160  [268800/285077]\n",
      "loss: 16.102079  [275200/285077]\n",
      "loss: 16.886423  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 15.043478 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 18.464458  [    0/285077]\n",
      "loss: 16.048462  [ 6400/285077]\n",
      "loss: 18.018286  [12800/285077]\n",
      "loss: 16.446236  [19200/285077]\n",
      "loss: 15.990431  [25600/285077]\n",
      "loss: 17.450264  [32000/285077]\n",
      "loss: 17.427214  [38400/285077]\n",
      "loss: 16.388891  [44800/285077]\n",
      "loss: 16.690950  [51200/285077]\n",
      "loss: 15.821966  [57600/285077]\n",
      "loss: 16.348675  [64000/285077]\n",
      "loss: 13.941989  [70400/285077]\n",
      "loss: 14.081703  [76800/285077]\n",
      "loss: 18.175024  [83200/285077]\n",
      "loss: 18.713877  [89600/285077]\n",
      "loss: 15.098248  [96000/285077]\n",
      "loss: 17.101479  [102400/285077]\n",
      "loss: 16.528980  [108800/285077]\n",
      "loss: 17.490736  [115200/285077]\n",
      "loss: 17.207933  [121600/285077]\n",
      "loss: 14.055618  [128000/285077]\n",
      "loss: 15.197140  [134400/285077]\n",
      "loss: 18.362881  [140800/285077]\n",
      "loss: 15.888012  [147200/285077]\n",
      "loss: 16.487076  [153600/285077]\n",
      "loss: 17.042313  [160000/285077]\n",
      "loss: 19.333244  [166400/285077]\n",
      "loss: 17.531219  [172800/285077]\n",
      "loss: 16.789494  [179200/285077]\n",
      "loss: 16.681469  [185600/285077]\n",
      "loss: 17.069311  [192000/285077]\n",
      "loss: 19.130339  [198400/285077]\n",
      "loss: 17.358646  [204800/285077]\n",
      "loss: 17.552877  [211200/285077]\n",
      "loss: 15.972274  [217600/285077]\n",
      "loss: 17.236340  [224000/285077]\n",
      "loss: 17.728855  [230400/285077]\n",
      "loss: 15.026374  [236800/285077]\n",
      "loss: 16.634027  [243200/285077]\n",
      "loss: 16.908272  [249600/285077]\n",
      "loss: 16.848679  [256000/285077]\n",
      "loss: 19.160751  [262400/285077]\n",
      "loss: 17.877047  [268800/285077]\n",
      "loss: 15.562435  [275200/285077]\n",
      "loss: 18.056162  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 15.147454 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 17.192902  [    0/285077]\n",
      "loss: 16.097139  [ 6400/285077]\n",
      "loss: 18.508755  [12800/285077]\n",
      "loss: 15.988745  [19200/285077]\n",
      "loss: 16.617935  [25600/285077]\n",
      "loss: 15.628453  [32000/285077]\n",
      "loss: 17.944473  [38400/285077]\n",
      "loss: 15.063835  [44800/285077]\n",
      "loss: 15.915574  [51200/285077]\n",
      "loss: 15.246687  [57600/285077]\n",
      "loss: 15.576969  [64000/285077]\n",
      "loss: 16.118378  [70400/285077]\n",
      "loss: 18.188992  [76800/285077]\n",
      "loss: 15.518949  [83200/285077]\n",
      "loss: 18.374935  [89600/285077]\n",
      "loss: 20.159609  [96000/285077]\n",
      "loss: 16.993317  [102400/285077]\n",
      "loss: 18.483299  [108800/285077]\n",
      "loss: 16.049072  [115200/285077]\n",
      "loss: 15.913657  [121600/285077]\n",
      "loss: 18.564039  [128000/285077]\n",
      "loss: 17.117531  [134400/285077]\n",
      "loss: 17.608850  [140800/285077]\n",
      "loss: 20.561930  [147200/285077]\n",
      "loss: 17.537548  [153600/285077]\n",
      "loss: 16.863115  [160000/285077]\n",
      "loss: 16.786564  [166400/285077]\n",
      "loss: 16.475546  [172800/285077]\n",
      "loss: 15.013801  [179200/285077]\n",
      "loss: 18.676384  [185600/285077]\n",
      "loss: 16.567513  [192000/285077]\n",
      "loss: 18.311214  [198400/285077]\n",
      "loss: 17.145748  [204800/285077]\n",
      "loss: 16.867252  [211200/285077]\n",
      "loss: 18.549553  [217600/285077]\n",
      "loss: 16.590466  [224000/285077]\n",
      "loss: 19.492977  [230400/285077]\n",
      "loss: 17.850267  [236800/285077]\n",
      "loss: 18.867226  [243200/285077]\n",
      "loss: 17.842733  [249600/285077]\n",
      "loss: 17.705847  [256000/285077]\n",
      "loss: 18.481833  [262400/285077]\n",
      "loss: 16.203028  [268800/285077]\n",
      "loss: 16.160740  [275200/285077]\n",
      "loss: 16.611254  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.4%, Avg loss: 14.584384 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 16.916090  [    0/285077]\n",
      "loss: 16.399195  [ 6400/285077]\n",
      "loss: 15.200010  [12800/285077]\n",
      "loss: 17.490559  [19200/285077]\n",
      "loss: 16.645842  [25600/285077]\n",
      "loss: 17.109755  [32000/285077]\n",
      "loss: 16.761913  [38400/285077]\n",
      "loss: 15.442940  [44800/285077]\n",
      "loss: 19.515099  [51200/285077]\n",
      "loss: 19.628672  [57600/285077]\n",
      "loss: 15.078089  [64000/285077]\n",
      "loss: 18.852425  [70400/285077]\n",
      "loss: 15.198044  [76800/285077]\n",
      "loss: 16.088207  [83200/285077]\n",
      "loss: 17.776701  [89600/285077]\n",
      "loss: 22.567139  [96000/285077]\n",
      "loss: 18.278542  [102400/285077]\n",
      "loss: 15.512493  [108800/285077]\n",
      "loss: 17.216860  [115200/285077]\n",
      "loss: 17.065804  [121600/285077]\n",
      "loss: 16.416647  [128000/285077]\n",
      "loss: 15.277760  [134400/285077]\n",
      "loss: 15.876641  [140800/285077]\n",
      "loss: 18.193333  [147200/285077]\n",
      "loss: 18.232126  [153600/285077]\n",
      "loss: 17.113068  [160000/285077]\n",
      "loss: 15.778130  [166400/285077]\n",
      "loss: 17.076128  [172800/285077]\n",
      "loss: 16.692818  [179200/285077]\n",
      "loss: 15.130496  [185600/285077]\n",
      "loss: 17.098907  [192000/285077]\n",
      "loss: 18.429184  [198400/285077]\n",
      "loss: 16.083235  [204800/285077]\n",
      "loss: 17.888426  [211200/285077]\n",
      "loss: 17.642494  [217600/285077]\n",
      "loss: 17.304825  [224000/285077]\n",
      "loss: 16.638790  [230400/285077]\n",
      "loss: 17.385437  [236800/285077]\n",
      "loss: 16.162527  [243200/285077]\n",
      "loss: 16.919235  [249600/285077]\n",
      "loss: 19.406187  [256000/285077]\n",
      "loss: 17.032829  [262400/285077]\n",
      "loss: 18.365133  [268800/285077]\n",
      "loss: 15.813823  [275200/285077]\n",
      "loss: 14.506177  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.5%, Avg loss: 14.539079 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 15.143690  [    0/285077]\n",
      "loss: 16.999302  [ 6400/285077]\n",
      "loss: 17.736744  [12800/285077]\n",
      "loss: 16.501347  [19200/285077]\n",
      "loss: 13.963055  [25600/285077]\n",
      "loss: 17.355738  [32000/285077]\n",
      "loss: 18.166523  [38400/285077]\n",
      "loss: 17.520468  [44800/285077]\n",
      "loss: 15.897902  [51200/285077]\n",
      "loss: 16.070503  [57600/285077]\n",
      "loss: 15.764419  [64000/285077]\n",
      "loss: 16.532803  [70400/285077]\n",
      "loss: 17.941383  [76800/285077]\n",
      "loss: 17.339449  [83200/285077]\n",
      "loss: 16.886124  [89600/285077]\n",
      "loss: 19.379316  [96000/285077]\n",
      "loss: 16.735523  [102400/285077]\n",
      "loss: 15.313726  [108800/285077]\n",
      "loss: 17.352936  [115200/285077]\n",
      "loss: 15.774540  [121600/285077]\n",
      "loss: 16.495426  [128000/285077]\n",
      "loss: 18.821568  [134400/285077]\n",
      "loss: 17.437403  [140800/285077]\n",
      "loss: 16.829262  [147200/285077]\n",
      "loss: 16.734346  [153600/285077]\n",
      "loss: 15.627085  [160000/285077]\n",
      "loss: 17.182480  [166400/285077]\n",
      "loss: 17.307446  [172800/285077]\n",
      "loss: 15.408573  [179200/285077]\n",
      "loss: 16.677490  [185600/285077]\n",
      "loss: 17.129028  [192000/285077]\n",
      "loss: 17.209520  [198400/285077]\n",
      "loss: 15.824831  [204800/285077]\n",
      "loss: 17.347540  [211200/285077]\n",
      "loss: 18.840080  [217600/285077]\n",
      "loss: 16.584118  [224000/285077]\n",
      "loss: 16.789812  [230400/285077]\n",
      "loss: 16.252022  [236800/285077]\n",
      "loss: 16.474739  [243200/285077]\n",
      "loss: 17.254286  [249600/285077]\n",
      "loss: 16.699085  [256000/285077]\n",
      "loss: 16.003231  [262400/285077]\n",
      "loss: 16.519699  [268800/285077]\n",
      "loss: 15.918280  [275200/285077]\n",
      "loss: 14.990753  [281600/285077]\n",
      "Test Error: \n",
      " Accuracy: 219.4%, Avg loss: 14.731432 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_dataloader, model, loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23bae617c2fd34ee163a70eb68c606bb45cef6dd4a4adb773de7f513ccf39765"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('old-betsy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
